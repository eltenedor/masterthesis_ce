\section{Comparison of Solver Concepts}
  
\subsection{Parallel Performance}

In many cases, scientific code is used to solve complex problems regarding memory requirements to make calculation results available within short time. In both scenarios, code that is able to run in parallel can alleviate the mentioned challenges. Code that runs in parallel can allocate more memory resources which makes the calculation of complex problems feasible. If the code is scalable the program execution can be shortened by using more processors to solve a problem of constant size.

The solver framework that has been developed in the course of the present thesis, has been parallelized using the PETSc library. After introducing the used hardware and software, the central measures of parallel performance are presented. Then preliminary test results using low-level benchmarks are performed, which establish upper performance bounds on the parallel efficiency and the scalability of the developed solver framework. The results of the efficiency evaluation of the solver framework is presented in the last subsections.
\subsubsection{Employed Hardware and Software -- The Lichtenberg-High Performance Computer }

All performance analyses that are presented in this thesis were conducted on the Lichtenberg-High Performance Computer, also known as \emph{HHLR} (\emph{Hessischer Hochleistungsrechner}) \cite{hhlr}. The cluster consist of different sections according to the used hardware. Throughout the thesis, tests were performed using the first and the second MPI section of the cluster. The first section consists of 705 nodes of which each runs two Intel\textregistered Xeon\textregistered E5-2670 processors and offers 32GB of memory. The second section consists of 356 nodes of which each runs two Intel Xeon E5-2680 v3 processors and offers 64GB of memory. As interconnect for both sections FDR-14 InfiniBand is used.

All tests programs were compiled using the Intel compiler suite version 15.0.0 and the compiler options
\lstset{language=bash,
  commentstyle={\rmfamily\catcode`\$=11},
  columns=flexible,
  texcl,
  keepspaces,
  ,showspaces=false
  ,showstringspaces=false,
  }
\begin{lstlisting}
-O3 -xHost
\end{lstlisting}
As MPI implementation Open MPI version 1.8.2 was chosen. Furthermore the PETSc version 3.5.3 was configured using the options
\begin{lstlisting}
--with-blas-lapack-dir=/shared/apps/intel/2015/composer_xe_2015/mkl/lib/intel64/ \
--with-mpi-dir=/shared/apps/openmpi/1.8.2_intel \
COPTFLAGS="-O3 -xHost" \
FOPTFLAGS="-O3 -xHost" \
CXXOPTFLAGS="-O3 -xHost" \
--with-debugging=0 \
--download-hypre \
--download-ml
\end{lstlisting}
It should be noted that as the configurations options show, to maximize the efficiency of PETSc, a math kernel library should be used that has been optimized for the underlying hardware architecture as is in the case of the present thesis the Intel \emph{MKL} (\emph{Math Kernel Library}). It should be noted that also the Open MPI library has been compiled using the Intel compiler suite.

\subsubsection{Measures of Performance}

This section establishes the needed set of measures to evaluate the performance of a solver program, which will be used in the following sections. The first measure is the plain measure of runtime \(T_P\) taken by a computer to solve a given problem, where \(P \in \mathbb{N}\) denotes the number of involved processes. This so called \emph{wall-clock} time can be measured directly by calling subroutines of the underlying operating system and corresponds to the human perception of the time, that has passed. It must be noted, that this time does not correspond to the often mentioned \emph{CPU} time. In fact, CPU time is only one contributor to wall-clock time. Wall-clock time further contains the time needed for communication and I/O and hence considers idle states of the processor. On the other side CPU time only considers the time in which the processor is actively working. This makes wall-clock time not only a more complete but also more accurate time measure when dealing with parallel processors, since processor idle times due to communication are actively considered while neglected in CPU time.

While wall-clock time is an absolute measure that can be used to compare different solver programs, further relative measures are needed to evaluate the efficiency of one program regarding the parallelisation implementation. The main purpose of these measures is to attribute the different causes of degrading efficiency due to heavy parallelisation to the different contributing factors. A simple model \cite{ferziger02,schaefer99} considers three contributions, that form the total efficiency
\begin{displaymath}
  E^{tot}_P = E^{num}_P \cdot E^{par}_P \cdot E^{load}_P.
\end{displaymath}
\begin{itemize}
  \item[] The \emph{numerical efficiency}
  \begin{displaymath} E^{num}_P := \frac{\operatorname{FLOPS}(1)}{P \cdot \operatorname{FLOPS}(P)}\end{displaymath} 
    considers the degradation of the efficiency of the underlying algorithm due to the parallelisation. Many efficient algorithms owe their efficiency to recursions inside the algorithm. In the process of decomposing this recursions, the efficiency of the algorithm degrades. It follows that this efficiency is completely independent of the underlying hardware.
  \item[] The \emph{parallel efficiency}
    \begin{displaymath} E^{par}_P :=\frac{\operatorname{TIME}(\text{parallel Algorithm on one processor})}{P \cdot \operatorname{TIME}(\text{parallel Algorithm on \(P\) processors})} \end{displaymath} 
      describes the impact of the need for inter process communication, if more than one processor is involved in the solution process. It should be noted, that this form of efficiency does explicitly exclude any algorithm related degrading, since the time measured corresponds to the exact same algorithms. It follows that the parallel efficiency only depends on the implementation of the communication and the hardware related latencies.
  \item[] The \emph{load balancing efficiency} 
    \begin{displaymath} E^{load}_P :=\frac{\operatorname{TIME}(\text{calculation on complete domain})}{P \cdot \operatorname{TIME}(\text{calculation on biggest subdomain})} \end{displaymath}
       is formed by the quotient of the wall times needed for the complete problem domain and partial solves on subdomains. This measure does neither depend on hardware nor on the used implementation. Instead it directly relates to the size and partition of the grid. 
\end{itemize}

It is not possible to calculate all three efficiencies at the same time using only plain wall clock time measurements of a given application.  Different solver configurations have to be used to calculate them separately. Since the focus of investigation of the present thesis does not lie on load balancing, for the remainder of the thesis \(E^{load}_P = 100\%  \) is assumed. This does not present a considerable drawback, since an ideal load balancing is easily obtainable nowadays by the use of sophisticated grid partitioning algorithms \cite{loadbalancing} REFERENCES. Using identical algorithms for different numbers of involved processes implicitly achieves \(E^{num}_P = 100 \%\). In this case the parallel efficiency of an application can be measured through the quotient of the needed wall clock time. To measure the numerical efficiency of an algorithm the respective hardware counters have to be evaluated. This can be done using the built in log file functionality of PETSc as presented in section REFERENCE. Hence the determination of numerical efficiency does not rely on wall clock time.

Another common performance measure is the \emph{Speed-Up}
\begin{displaymath}
  S_P = \frac{T_1}{T_P} = P \cdot E^{tot}_P.
\end{displaymath}
Speedup and parallel efficiency characterize the parallel scalability of an application and determine the regimes of efficient use of hardware resources.

\subsubsection{Preliminary Upper Bounds on Performance -- The STREAM Benchmark}

Scientific applications that solve partial differential equations rely on sparse matrix computations, which usually exhibit the sustainable memory bandwidth as bottleneck with respect to the runtime performance of the program \cite{hager11}. The purpose of this section is to establish a frame in terms of an upper bound on performance in which the efficiency of the developed solver framework can be evaluated critically. As common measure for the maximum sustainable bandwidth, low-level benchmarks can be used, which focus on evaluating specific properties of the deployed hardware architecture. In this case the STREAM benchmark suite \cite{mccalpin07,mccalpin95} provides apt tests, which are designed to work with data sets that exceed the cache size of the involved processor architecture. This forces the processors to stream the needed data directly from the memory instead of reusing the data residing in their caches. These types of tests can be used to calculate an upper bound on the memory bandwidth for the CAFFA framework.

In terms of parallel scalability, the STREAM benchmark can also be used as an upper performance bound. According to \cite{petsc-web-page} the parallel performance of memory bandwidth limited codes correlates with the parallel performance of the STREAM benchmark, i.e. a scalable increase in memory bandwidth is necessary for scalable application performance. The intermediate results of the benchmark can then be used to test different configurations that bind hardware resources to the involved processes. %Before presenting results the different binding configurations will be explained.

%The first configuration sequentially binds the processes to the cores beginning on the first socket. When every core has a bound process the binding algorithm binds the following processes to cores of the second socket. The second configuration binds the processes in a round robin manner regarding the sockets. This configuration in difference to the second configuration binds one process to three cores. Figures \ref{fig:binding1},\ref{fig:binding2} and \ref{fig:binding3} demonstrate the different binding options for two sockets and processors with twelve cores each, when eight processes are to be bound to the resources.

%\begin{figure}[h]
%  \centering
%  \label{fig:binding1}
%    \input{./img/map2.tikz.tex}
%    \centering{}
%  \caption{Default binding using Open MPI on a node with two sockets and processors with each twelve cores}
%\end{figure}
%
%\begin{figure}[h]
%  \centering
%  \label{fig:binding2}
%    \input{./img/map.tikz.tex}
%    \centering{}
%  \caption{Process binding using Open MPI and map-by ppr:8:node map-by ppr:4:socket on a node with two sockets and processors with each twelve cores}
%\end{figure}
%
%
%\begin{figure}[h]
%  \centering
%  \label{fig:binding3}
%    \input{./img/map3.tikz.tex}
%    \centering{}
%  \caption{Process binding using Open MPI and map-by ppr:8:node map-by ppr:4:socket:PE=3 on a node with two sockets and processors with each twelve cores}
%\end{figure}

\begin{figure} \centering
  \pgfplotsset{every axis/.append style={
      font=\large,
      line width=1pt,
  tick style={line width=0.8pt}}}
\begin{tikzpicture}
  \pgfplotsset{every axis legend/.append style={
      at={(0.5,1.03)},
  anchor=south}}
\begin{axis}[
    ylabel={Sustainable memori bandwidth MB/s},
    xlabel={Number of bound cores},
    xtick={1,4,8,12,16},
    %ytick={1.7e-003,1.75e-3,1.8e-003,1.85e-3},
    %yticklabels={1.7E-3,1.75E-3,1.8E-3,1.85E-3},
    %ymin=1.65e-003,ymax=1.9e-003,
    xmin=0,xmax=17,
    %ymin=0.5e4,ymax=1.3e5,
    ymin=0,ymax=1.3e5,
    %legend pos=outer north east,
    %height=20cm,width=10cm
    height=12cm,width=15cm,
    grid=major,
    ]
    \addplot file {./files/mpi1.def};
    \addplot file {./files/mpi1.op1};
    \addplot file {./files/mpi1.core};
    \addlegendentry{Default Binding};
    \addlegendentry{map-by ppr:8:node map-by ppr:4:socket}
    \addlegendentry{map-by core}
\end{axis}
\end{tikzpicture}
\caption{Sustainable memory bandwidth as determined by the STREAM benchmark (Triad) for different process binding options on one node of the MPI1 section}
\label{fig:mpi1band}
\end{figure}

\begin{figure} \centering
  \pgfplotsset{every axis/.append style={
      font=\large,
      line width=1pt,
  tick style={line width=0.8pt}}}
\begin{tikzpicture}
  \pgfplotsset{every axis legend/.append style={
      at={(0.5,1.03)},
  anchor=south}}
\begin{axis}[
    ylabel={Sustainable memory bandwidth MB/s},
    xlabel={Number of processes},
    xtick={1,4,8,12,16,20,24},
    %ytick={1.7e-003,1.75e-3,1.8e-003,1.85e-3},
    %yticklabels={1.7E-3,1.75E-3,1.8E-3,1.85E-3},
    %ymin=1.65e-003,ymax=1.9e-003,
    xmin=0,xmax=25,
    %ymin=0.5e4,ymax=1.3e5,
    ymin=0,ymax=1.3e5,
    height=12cm,width=15cm,
    grid=major,
    ]
    \addplot file {./files/mpi2.def};
     \addlegendentry{Default Binding};
     \addplot file {./files/mpi2.op1};
     \addlegendentry{map-by ppr:8:node map-by ppr:4:socket};
     %\addplot file {./files/mpi2.op2};
     %\addlegendentry{map-by ppr:8:node map-by ppr:4:socket:PE=3};
     \addplot file {./files/mpi2.op3};
     \addlegendentry{map-by ppr:4:node map-by ppr:4:socket:PE=6};
     \addplot file{./files/mpi2.core};
    \addlegendentry{map-by core}
     %\addplot file {./files/mpi2.op4};
     % \addlegendentry{map-by ppr:6:node map-by ppr:4:socket:PE=4};
\end{axis}
\end{tikzpicture}
\caption{Sustainable memory bandwidth as determined by the STREAM benchmark (Triad) for different process binding options on one node of the MPI2 section}
\label{fig:mpi2band}
\end{figure}

As can be seen from figure \ref{fig:mpi1band} and \ref{fig:mpi2band}, the scaling of the sustainable bandwidth behaves rather erratic, such that for process counts up until eight for the MPI1 section no reliable results can be obtained from the STREAM benchmark. It is assumed that this kind of behaviour is due to the automatic turbo boost the deployed processors apply, which cannot be controlled other than by turning it off. For the following performance analyses with respect to parallel performance it is desirable to minimize this kind of unpredictable side effects. Since all performance measures are relative but the plain measurement of wall clock time the reference value for the following performance measurements will be taken from the program execution for the maximum number of processes that can be bound without overlap to one deployed socket.

It should be taken into account that the STREAM benchmark used to test the present architecture was provided by the PETSc framework. Special effort has been made to adapt this benchmark to handle the described problem, by always using all available cores per socket but only running the benchmark on \(n\) processes while the other \(24 - n\) or \(16 - n\) processes run calculations that only use data from the L\(1\) cache of their corresponding core. The intention of this approach was to regulate the turbo boost effects by keeping a constant load on the processors for different number of STREAM benchmark processes. However this approach didn't generate results different from figures \ref{fig:mpi1band} and \ref{fig:mpi2band}.

\subsubsection{Speedup Measurement and Impact of Coupling Algorithm for Analytic Test Cases}

This section presents the results from the speedup measurements conducted on the supercomputer HHLR which has been presented in section \ref{sec:}. Furthermore the impact of the scaling algorithm on the running time of the corresponding solver program will be shown. All tests are conducted on the previously introduced HHLR cluster using the MPI1 and MEM2 section. The performance analyses will present the absolute time needed to solve for the flow of the analytical solution presented in section \ref{sec:manufacturedsolution}. In order to evaluate the parallel performance furthermore the speedup will be calculated based on the presented results. 

The performance of all solvers significantly depends on the performance of the linear solvers and the corresponding preconditioners used in the solution process. For all conducted tests the linear system for the discretized momentum balances will be solved with a GMRES (\emph{Generalized Minimal Residual}) \cite{saad} Krylov subspace solver and a incomplete LU-factorization as preconditioner. The pressure correction equation is solved by a CG (\emph{Conjugated Gradient}) solver with GAMG (\emph{Geometric Algebraic Multi Grid}) preconditioner. The coupled systems are solved by a GMRES Krylov subspace solver with GAMG preconditioning. The GAMG preconditioner was chosen so that a high numerical efficiency is obtained, constituting a basis for further parallel performance measurements. All further solver parameters are listed in table \ref{tab:performance}.

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density    & 1E-0 & $kg/m^3$      \\
    \rowcolor{black!00} Viscosity  & 1E-0 & K.A.  \\
    \rowcolor{black!00} Under relaxation u & 0.9 &  \\
    \rowcolor{black!20} Under relaxation p & 0.1 &  \\
    \rowcolor{black!00} Relative tolerance & 1E-8/1E-4 &
  \end{tabular}
  \caption{Characteristic problem properties used in the channel flow test case}
  \label{tab:performance}
\end{table}
 
\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
  \centering
  \begin{tikzpicture}
      \begin{loglogaxis}[
        xlabel=Number of Processes,
        ylabel=Wall Clock Time s,
        xtick={1,2,4,8,16,32,64,128,256,512},
        xticklabels={1,2,4,8,16,32,64,128,256,512},
      ]
      \addplot file {./files/seg.128};
      \addplot file {./files/cpld.gamg.128};
    \end{loglogaxis}
    \end{tikzpicture}
    \caption{Wall clock time comparison for segregated and fully-coupled solution algorithm solving for an analytical solution on a grid with $128^3$ cells}
  \end{minipage}%
  \hfil
  \begin{minipage}{0.45\textwidth}
  \centering
  \begin{tikzpicture}
      \begin{loglogaxis}[
        xlabel=Number of Processes,
        ylabel=Speedup,
        xtick={1,2,4,8,16,32,64,128,256,512},
        xticklabels={1,2,4,8,16,32,64,128,256,512},
        ytick={1,2,4,8,16,32,64,128,256,512},
        yticklabels={1,2,4,8,16,32,64,128,256,512},
        grid=major
      ]
      \addplot file {./files/speedup.seg.128};
      \addplot file {./files/speedup.cpld.128};
      \addplot[mark=none,black] file {./files/speedup.ideal};
    \end{loglogaxis}
    \end{tikzpicture}
    \caption{Speedup comparison for segregated and fully-coupled solution algorithm solving for an analytical solution on a grid with $128^3$ cells}
  \end{minipage}%
\end{figure}
From the comparison in figures REFERENCE it is evident that both algorithms are scalable and the coupled solution algorithm accelerates the solution process up to one order of magnitude. It has to be noted, that the presented results for the coupled solution algorithm were obtained using black-blox solvers from the PETSc framework. It is assumed that the performance of the coupled solution algorithm with respect to the needed walltime would benefit from multigrid solvers especially designed for the coupled system of linear equations. Examples for such spechial solvers can be found in \cite{darwish09,mangani14,klaij13}.

%\begin{figure}
%  \centering
%  \begin{minipage}{0.45\textwidth}
%  \centering
%  \begin{tikzpicture}
%      \begin{loglogaxis}[
%        xlabel=Number of Processes,
%        ylabel=Speedup,
%        xtick={1,2,4,8,16,32,64,128,256,512,1024},
%        xticklabels={1,2,4,8,16,32,64,128,256,512,1024},
%        ytick={1,2,4,8,16,32,64,128,256,512,1024},
%        yticklabels={1,2,4,8,16,32,64,128,256,512,1024},
%        grid=major
%      ]
%      \addplot file {./files/speedup.seg.256};
%      %\addplot file {./files/speedup.cpld.128};
%      \addplot[mark=none,black] file {./files/speedup.ideal};
%    \end{loglogaxis}
%    \end{tikzpicture}
%    \caption{Speedup comparison for segregated and fully-coupled solution algorithm solving for an analytical solution on a grid with $256^3$ cells}
%  \end{minipage}
%  \hfil
%  \begin{minipage}{0.45\textwidth}
%  \centering
%    \begin{tikzpicture}
%      \begin{loglogaxis}[
%        xlabel=Number of Processes,
%        ylabel=Wall Clock Time s,
%        xtick={1,4,16,64,256,1024},
%        xticklabels={1,4,16,64,256,1024},
%      ]
%      \addplot file {./files/seg.256};
%      \addplot file {./files/cpld.gamg.256};
%      \end{loglogaxis}
%    \end{tikzpicture}
%    \caption{Wall clock time comparison for segregated and fully-coupled solution algorithm solving for an analytical solution on a grid with $256^3$ cells}
%  \end{minipage}
%\end{figure}


\subsection{Realistic Testing Scenario -- Complex Geometry}

Fluid flow inside closed applications is a common situation in mechanical engineering. The flow through a channel with rectangular cross section can be seen as a simple test case that is a part of complex applications. This section compares the single process performance of the segregated and the fully coupled solution algorithm for a flow problem within a complex geometry. The geometry of the domain is based on a channel flow problem with square cross section, with the special property that inside the channel reside two obstacles with a square cross section of which one has been twisted against the other. Figure \ref{fig:sketch} shows a sketch of the problem domain. 

\begin{figure}
  \centering
  \includegraphics{./img/channel3d.pdf}
  \caption{Sketch of the channel flow problem domain}
  \label{fig:sketch}
\end{figure}

This case exercises all of the previously introduced boundary conditions for flow problems including the treatment of non-matching block boundaries. For the velocities at the inflow boundary the parabolic distribution
\begin{displaymath}
  \vec{u}(x_1,x_2,x_3) 
  =
\left[
  \begin{array}{ccc}
    u_1 \\
    u_2 \\
    u_3 
  \end{array}
\right]
  =
\left[
  \begin{array}{ccc}
    \frac{ 16 * 0.45 * x_2 * x_3 * \left( 0.41 - x_2 \right) * \left( 0.41 - x_3 \right)}{0.41^4}
    \\[0.9em]
    0 \\[0.3em]
    0 
  \end{array}
\right]
\end{displaymath}
was chosen. All the other problem parameters were chosen such that the flow problem resides in the regime of a non-turbulent stationary flow for which the presented solver framework has been developed. Table \ref{tab:channel} lists the remaining material and geometrical characteristics of the test case.

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density    & 1E-3 & $kg/m^3$      \\
    \rowcolor{black!00} Viscosity  & 1E-0 & K.A.  \\
    \rowcolor{black!20} Height     & 0.41 & m   \\
    \rowcolor{black!00} Length     & 2.5  & m  \\
    \rowcolor{black!20} Side length cube & 0.1  & m  \\
    \rowcolor{black!00} Under relaxation u & 0.9 &  \\
    \rowcolor{black!20} Under relaxation p & 0.1 &  \\
  \end{tabular}
  \caption{Characteristic problem properties used in the channel flow test case}
  \label{tab:channel}
\end{table}

The present test case shows one advantage of the treatment of block boundaries, which has been introduced in section REFERENCE. Since no assumptions on the geometry of a neighboring block are necessary each block can be constructed independently which increases the flexibility of the meshing of geometries. Furthermore, because of the fully implicit handling of block boundaries, the number of used blocks does not impact on the convergence properties of the deployed linear solvers. Figure \ref{fig:channel1} and figure \ref{fig:channel2} show the mesh at the left and right bounding walls. It is evident that this mesh leads to non-trivial transitions between the blocks. Figure \ref{fig:blocking} shows the domain decomposition into structured grid blocks around the two obstacles within the problem domain and emphasizes the need for accurate handling of non-matching block boundaries.

\begin{figure}
  \centering
  \label{fig:channel1}
  \input{./img/channel.tikz.tex}
  \caption{East boundary of the numerical grid for the channel flow problem}
\end{figure}

%\begin{figure}
%  \centering
%  \label{fig:channel2}
%  \input{./img/channel2.tikz.tex}
%  \caption{West boundary of the numerical grid for the channel flow problem }
%\end{figure}

\begin{figure}
   \label{fig:cellvertex}
   \centering
    \subfigure{
    \begin{minipage}[1\width]{0.4\textwidth}%
      \input{./img/blocking2.tikz.tex}
      \raggedleft{}
    \end{minipage}}\qquad
    \subfigure{
    \begin{minipage}[1\width]{0.4\textwidth}%
      \input{./img/blocking.tikz.tex}
      \raggedleft{}
    \end{minipage}}
    \caption{Blocking for the two different obstacles within the problem domain of the channel flow}
\end{figure}

The solution of the linear systems resulting from the discretization of the problem takes up more time for the coupled solution algorithm than in the segregated coupled algorithm. For small problem sizes the additional overhead for the solution methods for linear systems and the property that the segregated solution algorithm does not need many outer iterations to converge leads to the conclusion that moderate to big problems with respect to the number of involved unknowns are necessary for the coupled solution algorithm to dominate through performance. 

In contrast to the segregated algorithm, the fully coupled solution algorithm achieves an approximately constant amount of needed outer iterations, independent of the number of involved unknowns. The tests regarding the weak scaling of the coupled solution algorithm emphasize this property. Table \ref{tab:channelcompare} compares the measured wall clock time for different numbers of unknowns. The mesh shown in \ref{fig:channel1} and \ref{fig:channel2} was generated for the first number of unknowns and successively refined to achieve higher mesh resolutions. The two other numbers result from up to three times bisectioning the mesh in each direction, every time scaling the number of unknowns by a factor of eight. The tests were conducted on the formerly presented HHLR cluster, using the MPI2 and MEM2 section. 

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{lcccc}\toprule
    No. of Unknowns & Seg. - time & Cpld - time & Seg. - its & Cpld - its \\
    \midrule
    \rowcolor{black!20} 75768    & 0.2226E+02 & 0.3645E+02 & 151  & 67 \\
    \rowcolor{black!00} 408040   & 0.4053E+03 & 0.1500E+03 & 355  & 42 \\
    \rowcolor{black!20} 2611080  & 1.1352E+05 & 0.3105E+04 & 1592 & 39 \\
  \end{tabular}
  \caption{Performance analysis results of the channel flow problem}
  \label{tab:channelcompare}
\end{table}

The timing results show, that already after the first grid refinement step the fully coupled solution algorithm performs better with respect to the needed wall clock time for computation. This effect is even more clearly visible for higher mesh resolutions.

\subsection{Classical Benchmarking Case -- Heat-Driven Cavity Flow}

This section deals with the evaluation and comparison not only of the pressure-velocity coupling but also of the velocity-to-temperature coupling through the Boussinesq approximation and the temperature-velocity/pressure coupling through the Newton-Raphson linearization of the convective term in the temperature equation. For this the standard heat-driven cavity flow \cite{christon02,vahl83} is adapted for a three dimensional domain and the material and geometric parameters are chosen such that a non-turbulent stationary flow exists, which means that the solution lies within the regime of the approximations made by the solver.

Essential for this benchmarking case is the nature of the flow. The fluid motion is a consequence of the effect of volume forces caused by temperature differences in the solution domain, hence the mathematical problem exhibits a strong coupling bewteen the involved variables velocity, pressure and temperature. This relation is represented by the Rayleigh and Prandtl number. It is assumed that for this kind of flow the fully implicit treatment of the temperature coupling will yield further benefits with respect to wall time, compared with solution approaches that solve for the temperature separately. Table \ref{tab:cavity} lists the geometrical and solver parameters used for the performance analysis of this section.

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density    & 1E-3 & $kg/m^3$      \\
    \rowcolor{black!00} Viscosity  & 1E-0 & K.A.  \\
    \rowcolor{black!20} Height     & 0.41 & m   \\
    \rowcolor{black!00} Length     & 2.5  & m  \\
    \rowcolor{black!20} Side length cube & 0.1  & m  \\
    \rowcolor{black!00} Under relaxation u & 0.9 &  \\
    \rowcolor{black!20} Under relaxation p & 0.1 &  \\
    \rowcolor{black!00} Relative tolerance & 1E-8/1E-4 &
  \end{tabular}
  \caption{Characteristic problem properties used in the channel flow test case}
  \label{tab:cavity}
\end{table}

The tests were conducted on the formerly presented HHLR cluster, using the MPI2 and MEM2 section. For the tests involving higher resolutions the relative tolerance for convergence was increased to $1E-4$. Table \ref{tab:cavitycompare}

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{cccc}\toprule
    Resolution & Solver configuration & Time & Non-linear it. \\
    \midrule
    \rowcolor{black!20}\multirow{4}{*}{}            & SEG    & 0.3719E+02 & 203 \\
    \rowcolor{black!20}                             & CPLD   & 0.6861E+02 & 62  \\
    \rowcolor{black!20}                             & TCPLD  & 0.1012E+03 & 31  \\
    \rowcolor{black!20} \multirow{-4}{*}{32x32x32}  & NRCPLD & 0.2153E+02 & 22  \\ %\hline
    %
    \rowcolor{black!00}\multirow{4}{*}{}            & SEG    & 0.1997E+04 &  804 \\
    \rowcolor{black!00}                             & CPLD   & 0.7687E+03 &  63  \\
    \rowcolor{black!00}                             & TCPLD  & 0.1278E+04 &  59  \\
    \rowcolor{black!00} \multirow{-4}{*}{64x64x64}  & NRCPLD & 0.4240E+03 &  17  \\ %\hline
    %
    \rowcolor{black!20}\multirow{4}{*}{}               & SEG    & 0.5197E+05 &  3060 \\
    \rowcolor{black!20}                                & CPLD   & 0.1860E+05 &  74   \\
    \rowcolor{black!20}                                & TCPLD  & 0.1950E+05 &  50   \\
    \rowcolor{black!20} \multirow{-4}{*}{128x128x128}  & NRCPLD & 0.6155E+04 &  18   \\ %\hline
    %
    \rowcolor{black!00}\multirow{4}{*}{}               & SEG    &  & \\
    \rowcolor{black!00}                                & CPLD   &  & \\
    \rowcolor{black!00}                                & TCPLD  &  & \\
    \rowcolor{black!00} \multirow{-4}{*}{256x256x256}  & NRCPLD &  & \\ %\hline
    %\rowcolor{black!00}\multirow{3}{*}{} &  &  &  \\\rowcolor{black!00} &  &   &  \\\rowcolor{black!00} \multirow{-3}{*}{16x16x16}& &  &  \\ \hline
  \end{tabular}
  \caption{Performance analysis results of the heated cavity flow problem}
  \label{tab:cavitycompare}
\end{table}

The presented results are in good agreement with \cite{vakilipour12}, which show monotonic decrease of the number of iterations with increased implicit coupling. It is notable that the pressure velocity coupling is responsible for the biggest decrease in the number of non-linear iterations. Different to the results presented in section \cite{sec:channel} this does not yield significant performance benefits with respect to wall-clock time.  In order to achieve the benefits of a fully coupled solution algorithm the coupling has to be increased to also involve temperature-to-velocity/pressure and velocity-to-temperature coupling. 

Furthermore it can be noticed that the solely use of velocity-to-temperature coupling does not result in benefits compared to the coupled solution for velocities and pressure combined with a decoupled solve for the temperature equation. This is accredited to the treatment of the non-linearity of the temperature equation in the TCPLD solver configuration. Even though the momentum balances implicitly use the temperature from the next iteration, the temperature equation does not use the velocities of the next iteration. Instead the convective fluxes are calculated with the velocities from the previous iteration. Even though the implicit velocity-to-temperature coupling reduces the number of needed non-linear iterations an increase in the needed wall-clock time for computation can be noticed. This is attributed to the augmented costs during the application of the linear solver algorithm as a result of the additional degree of freedom that the linear system embraces. The necessary number of outer iterations is higher than in the NRCPLD configuration since the convective fluxes in the temperature equation are not coupled to the momentum balances which degrades the convergence of the non-linear iteration process.

It can be concluded that in order to profit from the benefits of implicit coupling the key lays in the temperature-to-velocity/pressure coupling. However, it should be noted that the implicit consideration of the corresponding terms furthermore increases the amount of memory needed for computation and hence does not come without drawbacks.

