\section{Comparison of Solver Concepts}
\label{sec:compare}

This section conducts the performance analysis of both developed solvers. In the first part of this section the used hardware and software are described, furthermore important performance measures are introduced. After performing a preliminary benchmark to measure the sustainable bandwidth, the Speed-Up is measured for both solver programs. The second part of this section tests the performance of the algorithm on a test case with complex geometry and on the heated cavity benchmark.
  
\subsection{Parallel Performance}

In many cases, scientific code is used to solve complex problems regarding memory requirements or to make calculation results available within short time. In both scenarios, code that is able to run in parallel can alleviate the mentioned challenges. Code that runs in parallel can allocate more memory resources which makes the calculation of complex problems feasible. If the code is scalable the program execution can be shortened by involving more processes to solve a problem of constant size on the compatible hardware.

The solver framework that has been developed in the course of the present thesis, has been parallelized using the PETSc library. After introducing the used hardware and software, the central measures of parallel performance are presented. Then preliminary test results using low-level benchmarks are performed, which establish upper performance bounds on the parallel efficiency and the scalability of the developed solver framework. The results of the efficiency evaluation of the solver framework are presented in the last subsections.

\subsubsection{Employed Hardware and Software -- The Lichtenberg-High Performance Computer }
\label{sec:hhlr}

All performance analyses that are presented in this thesis were conducted on the Lichtenberg-High Performance Computer, also known as \emph{HHLR} (\emph{Hessischer Hochleistungsrechner}). The cluster consist of different sections according to the used hardware. Throughout the thesis, tests were performed using the first and the second MPI section of the cluster. The first section consists of 705 nodes of which each runs two Intel\textregistered Xeon\textregistered E5-2670 processors and offers 32GB of memory. The second section consists of 356 nodes of which each runs two Intel Xeon E5-2680 v3 processors and offers 64GB of memory. As interconnect for both sections FDR-14 InfiniBand is used.

All tests programs were compiled using the Intel compiler suite version 15.0.0 and the compiler options
\lstset{language=bash,
  commentstyle={\rmfamily\catcode`\$=11},
  columns=flexible,
  texcl,
  keepspaces,
  ,showspaces=false
  ,showstringspaces=false,
  }
\begin{lstlisting}
-O3 -xHost
\end{lstlisting}
As MPI implementation Open MPI version 1.8.2 was chosen. Furthermore the PETSc version 3.5.3 was configured using the options
\begin{lstlisting}
--with-blas-lapack-dir=/shared/apps/intel/2015/composer_xe_2015/mkl/lib/intel64/ \
--with-mpi-dir=/shared/apps/openmpi/1.8.2_intel \
COPTFLAGS="-O3 -xHost" \
FOPTFLAGS="-O3 -xHost" \
CXXOPTFLAGS="-O3 -xHost" \
--with-debugging=0 \
--download-hypre \
--download-ml
\end{lstlisting}
It should be noted that as the configurations options show, to maximize the efficiency of PETSc, a math kernel library should be used that has been optimized for the underlying hardware architecture. Is in the case of the present thesis the Intel \emph{MKL} (\emph{Math Kernel Library}) has been used. The Open MPI library was compiled using Intel compilers as well.

\subsubsection{Measures of Performance}

This section establishes the needed set of measures to evaluate the performance of a solver program, which will be used in the following sections. The first measure is the plain measure of runtime \(T_P\) taken by a computer to solve a given problem, where \(P \in \mathbb{N}\) denotes the number of involved processes. This so called \emph{wall-clock} time can be measured directly by calling subroutines of the underlying operating system and corresponds to the human perception of the time, that has passed. It must be noted, that this time does not correspond to the often mentioned \emph{CPU} time. In fact, CPU time is only one contributor to wall-clock time. Wall-clock time further contains the time needed for communication and I/O and hence considers idle states of the processor. On the other side CPU time only considers the time in which the processor is actively working. This makes wall-clock time not only a more complete but also more accurate time measure when dealing with parallel processors, since processor idle times due to communication are actively considered while neglected in CPU time.

While wall-clock time is an absolute measure that can be used to compare different solver programs, further relative measures are needed to evaluate the efficiency of one program regarding the parallelisation implementation. The main purpose of these measures is to attribute the different causes of degrading efficiency due to heavy parallelisation to the different contributing factors. A simple model \cite{ferziger02,schaefer99} considers three contributions, that form the total efficiency
\begin{displaymath}
  E^{tot}_P = E^{num}_P \cdot E^{par}_P \cdot E^{load}_P.
\end{displaymath}
\begin{itemize}
  \item[] The \emph{numerical efficiency}
  \begin{displaymath} E^{num}_P := \frac{\operatorname{FLOPS}(1)}{P \cdot \operatorname{FLOPS}(P)}\end{displaymath} 
    considers the degradation of the efficiency of the underlying algorithm due to the parallelisation. Many efficient algorithms owe their efficiency to recursions inside the algorithm. In the process of decomposing this recursions, the efficiency of the algorithm degrades. It follows that this efficiency is completely independent of the underlying hardware.
  \item[] The \emph{parallel efficiency}
    \begin{displaymath} E^{par}_P :=\frac{\operatorname{TIME}(\text{parallel Algorithm on one processor})}{P \cdot \operatorname{TIME}(\text{parallel Algorithm on \(P\) processors})} \end{displaymath} 
      describes the impact of the need for inter process communication, if more than one processor is involved in the solution process. It should be noted, that this form of efficiency does explicitly exclude any algorithm related degrading, since the time measured corresponds to the exact same algorithms. It follows that the parallel efficiency only depends on the implementation of the communication and the hardware related latencies.
  \item[] The \emph{load balancing efficiency} 
    \begin{displaymath} E^{load}_P :=\frac{\operatorname{TIME}(\text{calculation on complete domain})}{P \cdot \operatorname{TIME}(\text{calculation on biggest subdomain})} \end{displaymath}
       is formed by the quotient of the wall times needed for the complete problem domain and partial solves on subdomains. This measure does neither depend on hardware nor on the used implementation. Instead it directly relates to the size and partition of the grid. 
\end{itemize}

It is not possible to calculate all three efficiencies at the same time using only plain wall-clock time measurements of a given application.  Different solver configurations have to be used to calculate them separately. Since the focus of investigation of the present thesis does not lie on load balancing, for the remainder of the thesis \(E^{load}_P = 100\%  \) is assumed. This does not present a considerable drawback, since an ideal load balancing is easily obtainable nowadays by the use of grid partitioning algorithms \cite{ferziger02}. Using identical algorithms for different numbers of involved processes implicitly, achieves \(E^{num}_P = 100 \%\). In this case the parallel efficiency of an application can be measured through the quotient of the needed wall-clock time. To measure the numerical efficiency of an algorithm the respective hardware counters have to be evaluated. This can be done using the built in log file functionality of PETSc as presented in section \ref{sec:petsc}. Hence the determination of numerical efficiency does not rely on wall-clock time.

Another common performance measure is the \emph{Speed-Up}
\begin{displaymath}
  S_P = \frac{T_1}{T_P} = P \cdot E^{tot}_P.
\end{displaymath}
Speed-Up and parallel efficiency characterize the parallel scalability of an application and determine the regimes of efficient use of hardware resources.

\subsubsection{Preliminary Upper Bounds on Performance -- The STREAM Benchmark}

Scientific applications that solve partial differential equations rely on sparse matrix computations, which usually exhibit the sustainable memory bandwidth as bottleneck with respect to the runtime performance of the program \cite{hager11}. The purpose of this section is to establish a frame in terms of an upper bound on performance in which the efficiency of the developed solver framework can be evaluated critically. As common measure for the maximum sustainable bandwidth, low-level benchmarks can be used, which focus on evaluating specific properties of the deployed hardware architecture. In this case the STREAM benchmark suite \cite{mccalpin07,mccalpin95} provides apt tests, which are designed to work with data sets that exceed the cache size of the involved processor architecture. This forces the processors to stream the needed data directly from memory instead of reusing the data residing in their caches. These types of tests can be used to calculate an upper bound on the memory bandwidth for the CAFFA framework.

In terms of parallel scalability, the STREAM benchmark can also be used as an upper performance bound. According to \cite{petsc-web-page} the parallel performance of memory bandwidth limited codes correlates with the parallel performance of the STREAM benchmark, i.e. a scalable increase in memory bandwidth is necessary for scalable application performance. The intermediate results of the benchmark can then be used to test different configurations that bind hardware resources to the involved processes. Figures \ref{fig:mpi1band} and \ref{fig:mpi2band} show the test results of the STREAM benchmark on the two different MPI sections of the HHRL. It should be noted that it is central for the results to be reproducible to always run the tests exclusively on each node, i.e. that no other process is running on the node. %Before presenting results the different binding configurations will be explained.

%The first configuration sequentially binds the processes to the cores beginning on the first socket. When every core has a bound process the binding algorithm binds the following processes to cores of the second socket. The second configuration binds the processes in a round robin manner regarding the sockets. This configuration in difference to the second configuration binds one process to three cores. Figures \ref{fig:binding1},\ref{fig:binding2} and \ref{fig:binding3} demonstrate the different binding options for two sockets and processors with twelve cores each, when eight processes are to be bound to the resources.

%\begin{figure}[h]
%  \centering
%  \label{fig:binding1}
%    \input{./img/map2.tikz.tex}
%    \centering{}
%  \caption{Default binding using Open MPI on a node with two sockets and processors with each twelve cores}
%\end{figure}
%
%\begin{figure}[h]
%  \centering
%  \label{fig:binding2}
%    \input{./img/map.tikz.tex}
%    \centering{}
%  \caption{Process binding using Open MPI and map-by ppr:8:node map-by ppr:4:socket on a node with two sockets and processors with each twelve cores}
%\end{figure}
%
%
%\begin{figure}[h]
%  \centering
%  \label{fig:binding3}
%    \input{./img/map3.tikz.tex}
%    \centering{}
%  \caption{Process binding using Open MPI and map-by ppr:8:node map-by ppr:4:socket:PE=3 on a node with two sockets and processors with each twelve cores}
%\end{figure}

\begin{figure} \centering
  \pgfplotsset{every axis/.append style={
      font=\large,
      line width=1pt,
  tick style={line width=0.8pt}}}
\begin{tikzpicture}
  \pgfplotsset{every axis legend/.append style={
      at={(0.5,1.03)},
  anchor=south}}
\begin{axis}[
    ylabel={Sustainable memory bandwidth MB/s},
    xlabel={Number of bound cores},
    xtick={1,4,8,12,16},
    %ytick={1.7e-003,1.75e-3,1.8e-003,1.85e-3},
    %yticklabels={1.7E-3,1.75E-3,1.8E-3,1.85E-3},
    %ymin=1.65e-003,ymax=1.9e-003,
    xmin=0,xmax=17,
    %ymin=0.5e4,ymax=1.3e5,
    ymin=0,ymax=1.3e5,
    %legend pos=outer north east,
    %height=20cm,width=10cm
    height=12cm,width=15cm,
    grid=major,
    ]
    \addplot file {./files/mpi1.def};
    \addplot file {./files/mpi1.op1};
    \addplot file {./files/mpi1.core};
    \addlegendentry{Default binding};
    \addlegendentry{map-by ppr:8:node map-by ppr:4:socket}
    \addlegendentry{map-by core}
\end{axis}
\end{tikzpicture}
\caption{Sustainable memory bandwidth as determined by the STREAM benchmark (Triad) for different process binding options on one node of the MPI1 section}
\label{fig:mpi1band}
\end{figure}

\begin{figure} \centering
  \pgfplotsset{every axis/.append style={
      font=\large,
      line width=1pt,
  tick style={line width=0.8pt}}}
\begin{tikzpicture}
  \pgfplotsset{every axis legend/.append style={
      at={(0.5,1.03)},
  anchor=south}}
\begin{axis}[
    ylabel={Sustainable memory bandwidth MB/s},
    xlabel={Number of processes},
    xtick={1,4,8,12,16,20,24},
    %ytick={1.7e-003,1.75e-3,1.8e-003,1.85e-3},
    %yticklabels={1.7E-3,1.75E-3,1.8E-3,1.85E-3},
    %ymin=1.65e-003,ymax=1.9e-003,
    xmin=0,xmax=25,
    %ymin=0.5e4,ymax=1.3e5,
    ymin=0,ymax=1.3e5,
    height=12cm,width=15cm,
    grid=major,
    ]
    \addplot file {./files/mpi2.def};
     \addlegendentry{Default Binding};
     \addplot file {./files/mpi2.op1};
     \addlegendentry{map-by ppr:8:node map-by ppr:4:socket};
     %\addplot file {./files/mpi2.op2};
     %\addlegendentry{map-by ppr:8:node map-by ppr:4:socket:PE=3};
     \addplot file {./files/mpi2.op3};
     \addlegendentry{map-by ppr:4:node map-by ppr:4:socket:PE=6};
     \addplot file{./files/mpi2.core};
    \addlegendentry{map-by core}
     %\addplot file {./files/mpi2.op4};
     % \addlegendentry{map-by ppr:6:node map-by ppr:4:socket:PE=4};
\end{axis}
\end{tikzpicture}
\caption{Sustainable memory bandwidth as determined by the STREAM benchmark (Triad) for different process binding options on one node of the MPI2 section}
\label{fig:mpi2band}
\end{figure}

As can be seen from figure \ref{fig:mpi1band} and \ref{fig:mpi2band}, the scaling of the sustainable bandwidth behaves rather erratic, such that for process counts up until eight for the MPI1 section no reliable results can be obtained from the STREAM benchmark. It is assumed that this kind of behaviour is due to the automatic turbo-boost the deployed processors apply, which cannot be controlled other than by turning it off directly from the nodes from the BIOS. For the following performance analyses with respect to parallel performance it is desirable to minimize this kind of unpredictable side effects. Since all performance measures are relative but the plain measurement of wall-clock time, the reference value for the following performance measurements will be taken from the program execution for the maximum number of processes that can be bound without overlap to one deployed socket.

It should be taken into account that the STREAM benchmark used to test the present architecture was provided by the PETSc framework. Special effort has been made to adapt this benchmark to handle the described problem, by always using all available cores per socket but only running the benchmark on \(n\) processes while the other \(24 - n\) or \(16 - n\) processes run calculations that only use data from the L\(1\)-cache of their corresponding core. The intention of this approach was to regulate the turbo-boost effects by keeping a constant load on the processors for different numbers of STREAM benchmark processes. However this approach didn't generate results different from figures \ref{fig:mpi1band} and \ref{fig:mpi2band}.

\subsubsection{Speed-Up Measurement and Impact of Coupling Algorithm for Analytic Test Case}

This section presents the results from the Speed-Up measurements conducted on the supercomputer HHLR which has been presented in section \ref{sec:hhlr}. Furthermore the impact of the scaling algorithm on the running time of the corresponding solver program will be shown. This property of applications if often termed \emph{strong} scalability \cite{hager11}. All tests are conducted on the previously introduced HHLR cluster using the MPI1 section. The performance analyses will present the absolute time needed to solve for the flow of the analytical solution presented in section \ref{sec:manufacturedsolution}. In order to evaluate the parallel performance furthermore the Speed-Up will be calculated based on the presented results. 

The performance of all solvers significantly depends on the performance of the linear solvers and the corresponding preconditioners used in the solution process. For all conducted tests the linear system for the discretized momentum balances will be solved with a GMRES (\emph{Generalized Minimal Residual}) \cite{saad86} Krylov subspace solver and an incomplete LU-factorization as preconditioner. The pressure correction equation is solved by a CG (\emph{Conjugated Gradient}) solver \cite{hestenes52} with GAMG (\emph{Geometric Algebraic Multi Grid}) preconditioner. The coupled systems are solved by a GMRES Krylov subspace solver with GAMG preconditioning. The GAMG preconditioner was chosen so that a high numerical efficiency is obtained, constituting a basis for further parallel performance measurements based on plain wall-clock time measurements. All further solver parameters are listed in table \ref{tab:performance}.

\begin{table}[h!]\centering
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density    & 1E-0 & $kg/m^3$      \\
    \rowcolor{black!00} Viscosity  & 1E-0 & $Ns/m^2$  \\
    \rowcolor{black!00} Under-relaxation u & 0.9 &  \\
    \rowcolor{black!20} Under-relaxation p & 0.1 &  \\
    \rowcolor{black!00} Relative tolerance & 1E-8/1E-4 &
  \end{tabular}
  \caption{Characteristic problem properties used in the performance measurements solving for an analytic solution}
  \label{tab:performance}
\end{table}
 
\begin{figure}
  %\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
  \begin{center}
  \begin{tikzpicture}
      \begin{loglogaxis}[
        legend columns=-1,
        legend entries={SIMPLE algorithm,Fully coupled algorithm},
        legend to name=named,
        xlabel=Number of processes,
        ylabel=Wall-clock time s,
        xtick={1,2,4,8,16,32,64,128,256,512,1024},
        xticklabels={1,2,4,8,16,32,64,128,256,512,1024},
        xmin=1,xmax=1200,
      ]
      \addplot[color=black,mark=*] file {./files/seg.128};
      \addplot[color=black,mark=square*] file {./files/cpld.gamg.128};
    \end{loglogaxis}
  % \caption{Wall-clock time comparison for segregated and fully coupled solution algorithm solving for an analytical solution on a grid with $128^3$ cells}
  % \label{fig:wall}
  \end{tikzpicture}
  %
  \begin{tikzpicture}
      \begin{loglogaxis}[
        xlabel=Number of processes,
        ylabel=Speed-Up,
        xtick={1,2,4,8,16,32,64,128,256,512,1024},
        xticklabels={1,2,4,8,16,32,64,128,256,512,1024,},
        ytick={1,2,4,8,16,32,64,128,256,512},
        yticklabels={1,2,4,8,16,32,64,128,256,512},
        grid=major,
        xmin=8,xmax=1200,
      ]
      \addplot[color=black,mark=*] file {./files/speedup.seg.128};
      \addplot[color=black,mark=square*] file {./files/speedup.cpld.128};
      \addplot[mark=none,black] file {./files/speedup.ideal};
    \end{loglogaxis}
    \end{tikzpicture}
    \\
  \ref{named}
  \end{center}
  \caption{Wall-clock time and Speed-Up comparison for segregated and fully coupled solution algorithm solving for an analytical solution on a grid with $128^3$ cells}
   \label{fig:speedup}
\end{figure}

From the comparison in figure \ref{fig:speedup} it is evident that both algorithms are scalable and the coupled solution algorithm accelerates the solution process up to one order of magnitude. It has to be noted, that the presented results for the coupled solution algorithm were obtained using black-box solvers from the PETSc framework. It is assumed that the performance of the coupled solution algorithm with respect to the needed wall-clock time would benefit from multigrid solvers especially designed for the coupled system of linear equations. Examples for such special solvers can be found in \cite{darwish09,mangani14,klaij13}. Another approach is represented by \emph{physics-based} solvers which take advantage of the matrix structure as shown in figure \ref{fig:nointerlacemat}. References \cite{brown12,mcinnes14} describe how PETSc can be used to construct physics based preconditioners in the context of multiphysics simulations.

%\begin{figure}
%  \centering
%  \begin{minipage}{0.45\textwidth}
%  \centering
%  \begin{tikzpicture}
%      \begin{loglogaxis}[
%        xlabel=Number of Processes,
%        ylabel=Speedup,
%        xtick={1,2,4,8,16,32,64,128,256,512,1024},
%        xticklabels={1,2,4,8,16,32,64,128,256,512,1024},
%        ytick={1,2,4,8,16,32,64,128,256,512,1024},
%        yticklabels={1,2,4,8,16,32,64,128,256,512,1024},
%        grid=major
%      ]
%      \addplot file {./files/speedup.seg.256};
%      %\addplot file {./files/speedup.cpld.128};
%      \addplot[mark=none,black] file {./files/speedup.ideal};
%    \end{loglogaxis}
%    \end{tikzpicture}
%    \caption{Speedup comparison for segregated and fully coupled solution algorithm solving for an analytical solution on a grid with $256^3$ cells}
%  \end{minipage}
%  \hfil
%  \begin{minipage}{0.45\textwidth}
%  \centering
%    \begin{tikzpicture}
%      \begin{loglogaxis}[
%        xlabel=Number of Processes,
%        ylabel=Wall-Clock Time s,
%        xtick={1,4,16,64,256,1024},
%        xticklabels={1,4,16,64,256,1024},
%      ]
%      \addplot file {./files/seg.256};
%      \addplot file {./files/cpld.gamg.256};
%      \end{loglogaxis}
%    \end{tikzpicture}
%    \caption{Wall clock time comparison for segregated and fully coupled solution algorithm solving for an analytical solution on a grid with $256^3$ cells}
%  \end{minipage}
%\end{figure}

\subsection{Weak Scaling Comparison of Coupled and Segregated Solution Algorithm}

It has been previously reported in \cite{darwish09,vakilipour12} that the coupled solution algorithm also scales well with the size of the problem domain with respect to the involved number of unknowns. It is known that this is not true for the SIMPLE algorithm as representative of the set of segregated solution algorithms. An algorithm that scales well with the problem size is apt to be tested on a benchmark that analyzes the so called \emph{weak} scaling of the implementation \cite{hager11}. Starting from a moderate number of unknowns for a tree-dimensional test case solving for the analytical solution presented in section \ref{sec:manufacturedsolution}, this benchmark will investigate how the running-time and the number of needed inner and outer iterations behaves when the computational load per process is held constant, while the number of involved processes is continuously increased. \emph{Weak} scaling benchmarks show, if it is possible to solve a more complex problem in the same time by involving more processes running the solver application. Each process will get assigned a block with \(32x32x32\) unknowns. The number of involved unknowns in each test run thus can be calculated as \( N = 32*32*32*PROC \). Figure \ref{fig:weak} compares the needed number of outer iterations and the corresponding wall-clock time for an implementation of the SIMPLE algorithm and the fully coupled solution algorithm.

\begin{figure}[h!]
  \begin{center}
  \begin{tikzpicture}
      \begin{loglogaxis}[
        legend columns=-1,
        legend entries={SIMPLE algorithm,Fully coupled algorithm},
        legend to name=named,
        xlabel=Number of processes,
        ylabel=Number of outer iterations,
        xtick={1,2,4,8,16,32,64,128,256,512,1024},
        xticklabels={1,2,4,8,16,32,64,128,256,512,1024,},
        %ytick={1,2,4,8,16,32,64,128,256,512},
        %yticklabels={1,2,4,8,16,32,64,128,256,512},
        %grid=major,
        xmin=8,xmax=1200,
      ]
      \addplot[color=black,mark=*] file {./files/weak.seg.iter};
      \addplot[color=black,mark=square*] file {./files/weak.iter};
    \end{loglogaxis}
    \end{tikzpicture}
    %
    \begin{tikzpicture}
      \begin{loglogaxis}[
        xlabel=Number of processes,
        ylabel=Wall-clock time s,
        xtick={1,2,4,8,16,32,64,128,256,512,1024},
        xticklabels={1,2,4,8,16,32,64,128,256,512,1024,},
        %ytick={1,2,4,8,16,32,64,128,256,512},
        %yticklabels={1,2,4,8,16,32,64,128,256,512},
        %grid=major,
        xmin=8,xmax=1200,
      ]
      \addplot[color=black,mark=*] file {./files/weak.seg.time};
      \addplot[color=black,mark=square*] file {./files/weak.time};
    \end{loglogaxis}
    \end{tikzpicture}
    \\
  \ref{named}
  \end{center}
  \caption{Comparison of the number of outer iterations and the corresponding execution wall-clock time needed to achieve a reduction of 1E-8 of the initial residual for different methods to resolve pressure-velocity coupling}
  \label{fig:weak}
\end{figure}

The comparison in figure \ref{fig:weak} shows that the fully coupled algorithm scales ideally with respect to the number of outer iterations, which remains almost constant throughout the conducted study. The number of iterations at most differed by 2 outer iterations. These differences are suspected to be caused by the loose tolerance criterion of residual reduction in each outer iteration. As an effect of accumulating rounding errors, some test cases did not accomplish the total tolerance criterion and thus needed to perform another additional outer iteration. From the wall-clock time measurements presented in figure \ref{fig:weak} it is evident that despite the approximately constant number of outer iterations the fully coupled solution algorithm did not scale ideally. Figure \ref{fig:weakinner} shows that weak scaling is prevented because the linear solver did not scale ideally. This shows that weak scaling cannot be obtained using black-box linear solvers and that further research regarding the development of special linear solvers for the systems resulting of a fully coupled discretization of the Navier-Stokes equations is needed.

\begin{figure}[h!]
  \begin{center}
  \begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=Number of processes,
        ylabel=Average number of inner iterations,
        xtick={1,2,4,8,16,32,64,128,256,512,1024},
        xticklabels={1,2,4,8,16,32,64,128,256,512,1024,},
      ]
      \addplot[color=black,mark=square*] file {./files/weak.inner};
      \end{semilogxaxis}
    \end{tikzpicture}
  \end{center}
  \caption{Average number of inner iterations for different process counts solving for an analytical solution with the fully coupled solution algorithm}
  \label{fig:weakinner}
\end{figure}

\subsection{Realistic Testing Scenario -- Complex Geometry}
\label{sec:channel}

Fluid flow inside closed applications is a common situation in mechanical engineering. The flow through a channel with rectangular cross section can be seen as a simple test case that is a part of complex applications. This section compares the single process performance of the segregated and the fully coupled solution algorithm for a flow problem within a complex geometry. The geometry of the domain is based on a channel flow problem with square cross section, with the special property that inside the channel reside two obstacles with a square cross section of which one has been twisted against the other. Figure \ref{fig:sketch} shows a sketch of the problem domain. 

\begin{figure}
  \centering
  \includegraphics{./img/channel3d.pdf}
  \caption{Sketch of the channel flow problem domain}
  \label{fig:sketch}
\end{figure}

This case exercises all of the previously introduced boundary conditions for flow problems including the treatment of non-matching block boundaries. For the velocities at the inflow boundary the parabolic distribution
\begin{displaymath}
  \vec{u}(x_1,x_2,x_3) 
  =
\left[
  \begin{array}{ccc}
    u_1 \\
    u_2 \\
    u_3 
  \end{array}
\right]
  =
\left[
  \begin{array}{ccc}
    \frac{ 16 * 0.45 * x_2 * x_3 * \left( 0.41 - x_2 \right) * \left( 0.41 - x_3 \right)}{0.41^4}
    \\[0.9em]
    0 \\[0.3em]
    0 
  \end{array}
\right]
\end{displaymath}
was chosen. At the outlet a Dirichlet pressure boundary condition was used. All other boundaries used a no-slip solid non-moving wall boundary condition. All problem parameters were chosen such that the flow problem resides in the regime of a non-turbulent stationary flow for which the presented solver framework has been developed. Table \ref{tab:channel} lists the remaining material and geometrical characteristics of the test case.

\begin{table}[h!]\centering
  \caption{Characteristic problem properties used in the channel flow test case}
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density            & 1E-0 & $kg/m^3$  \\
    \rowcolor{black!00} Viscosity          & 1E-3 & $Ns/m^2$  \\
    \rowcolor{black!20} Height             & 0.41 & m         \\
    \rowcolor{black!00} Length             & 2.5  & m         \\
    \rowcolor{black!20} Side length cube   & 0.1  & m   \\
    \rowcolor{black!00} Under-relaxation u & 0.9  &    \\
    \rowcolor{black!20} Under-relaxation p & 0.1  &    \\
    \rowcolor{black!00} Relative tolerance & 1E-8      &
  \end{tabular}
  \label{tab:channel}
\end{table}

Using the data from table \ref{tab:channel} and the maximal inflow velocity of $0.45m/s$ the Reynolds number \(Re\) can be calculated as
\begin{displaymath}
  Re = \frac{\rho \operatorname{mean}(u_1) l}{\mu} = 20,
\end{displaymath}
where \(\operatorname{mean}(u_1) = 0.45*\frac{4}{9} m/s = 0.2 m/s\). This shows that the flow resides in a laminar regime.

The present test case shows one advantage of the treatment of block boundaries, which has been introduced in section \ref{sec:blockboundaries}. Since no assumptions on the geometry of a neighboring block are necessary, the mesh for each block can be constructed independently which increases the flexibility of the meshing of geometries. Furthermore, because of the fully implicit handling of block boundaries, the number of used blocks does not negatively impact the convergence of the deployed linear solvers. Figure \ref{fig:channel1} shows the mesh at the left and right bounding walls. It is evident that this mesh leads to non-trivial transitions between the blocks. Figure \ref{fig:blocking} shows the domain decomposition into structured grid blocks around the two obstacles within the problem domain and emphasizes the need for accurate handling of non-matching block boundaries.

\begin{figure}
  \centering
  \input{./img/channel.tikz.tex}
  \caption{West and east boundary of the numerical grid for the channel flow problem}
  \label{fig:channel1}
\end{figure}

%\begin{figure}
%  \centering
%  \label{fig:channel2}
%  \input{./img/channel2.tikz.tex}
%  \caption{West boundary of the numerical grid for the channel flow problem }
%\end{figure}

\begin{figure}
   \centering
    \subfigure{
    \begin{minipage}{0.45\textwidth}%
      \input{./img/blocking2.tikz.tex}
      \raggedleft{}
    \end{minipage}}
    \hfil
    \subfigure{
    \begin{minipage}{0.45\textwidth}%
      \input{./img/blocking.tikz.tex}
      \raggedleft{}
    \end{minipage}}
    \caption{Blocking for the two different obstacles within the problem domain of the channel flow}
    \label{fig:blocking}
\end{figure}

The solution of the linear systems resulting from the discretization of the problem takes up more time for the coupled solution algorithm than in the segregated coupled algorithm. For small problem sizes the additional overhead for the solution methods for linear systems and the fact that the segregated solution algorithm for small number of unknowns does not need many outer iterations to converge leads to the conclusion that moderate to big problems with respect to the number of involved unknowns are necessary for the coupled solution algorithm to dominate through performance. 

In contrast to the segregated algorithm, the fully coupled solution algorithm achieves an approximately constant amount of needed outer iterations, independent of the number of involved unknowns. The tests regarding the weak scaling of the coupled solution algorithm emphasize this property. Table \ref{tab:channelcompare} compares the measured wall-clock time for different numbers of unknowns. The mesh shown in figure \ref{fig:channel1} was generated for the first number of unknowns and successively refined to achieve higher mesh resolutions. The two other numbers of unknowns result from up to two times bisectioning the mesh in each direction, every time scaling the number of unknowns by a factor of approximately eight. The tests were conducted on the formerly presented HHLR cluster, using the MPI2 section. 

\begin{table}[h!]\centering
  \caption{Performance analysis results of the channel flow problem for different numbers of unknowns comparint the segregated (SEG) to the fully coupled (CPLD) solution algorithm using one process on the MPI2 section of the HHLR supercomputer. }
\ra{1.3}
  \begin{tabular}{lcccc}\toprule
    No. of Unknowns & SEG - time & CPLD - time & SEG - its & CPLD - its \\
    \midrule
    \rowcolor{black!20} 75768    & 0.2226E+02 & 0.2674E+02 & 151  & 67 \\
    \rowcolor{black!00} 408040   & 0.4053E+03 & 0.1499E+03 & 355  & 42 \\
    \rowcolor{black!20} 2611080  & 1.1352E+05 & 0.3105E+04 & 1592 & 39 \\
  \end{tabular}
  \label{tab:channelcompare}
\end{table}

The timing results show, that already after the first grid refinement step the fully coupled solution algorithm performs better with respect to the needed wall-clock time for computation. This effect is even more clearly visible for higher mesh resolutions.

\subsection{Classical Benchmarking Case -- Heat-Driven Cavity Flow}

This section deals with the evaluation and comparison not only of the pressure-velocity coupling but also of the velocity-to-temperature coupling through the Boussinesq approximation and the temperature-velocity/pressure coupling through the Newton-Raphson linearization of the convective term in the temperature equation. For this the standard heat-driven cavity flow \cite{christon02,vahl83} is adapted for a three dimensional domain and the material and geometric parameters are chosen such that a non-turbulent stationary flow exists, which means that the solution lies within the regime of the approximations made by the solver. Figure \ref{fig:sketchcavity} shows an example for a temperature and velocity field obtained with the developed solver framework, solving for the heated cavity flow problem.

\begin{figure}[h!]
  \centering
  \subfigure{
  \begin{minipage}{0.45\textwidth}%
  \includegraphics[trim=3.5cm 5cm 0cm 5cm, scale=0.5,clip=true]{./img/cavity.pdf}
  \raggedleft{}
\end{minipage}}
\hfil
\subfigure{
\begin{minipage}{0.45\textwidth}
  \includegraphics[trim=3.5cm 4.5cm 0cm 5cm, scale=0.5,clip=true]{./img/cavityw.pdf}
\end{minipage} }
  \caption{Temperature field and velocity field in the coordinate direction of the gravitational force of the cavity flow problem for a cross section in the middle of the problem domain }
  \label{fig:sketchcavity}
\end{figure}

Essential for this benchmarking case is the nature of the flow. The fluid motion is a consequence of the effect of volume forces caused by temperature differences in the solution domain, hence the mathematical problem exhibits a strong coupling between the involved variables velocity, pressure and temperature. This relation is represented by the Rayleigh and Prandtl number. It is assumed that for this kind of flow the fully implicit treatment of the temperature coupling will yield further benefits with respect to wall time, compared with solution approaches that solve for the temperature separately. Table \ref{tab:cavity} lists the geometrical and solver parameters used for the performance analysis of this section.

\begin{table}[h!]\centering
  \caption{Characteristic problem properties used in the heated cavity flow test case}
\ra{1.3}
  \begin{tabular}{lcc}\toprule
    Property & Value & Unit \\
    \midrule
    \rowcolor{black!20} Density            & 1.19      & $kg/m^3$  \\
    \rowcolor{black!00} Viscosity          & 1.8E-5    & $Ns/m^2$  \\
    \rowcolor{black!20} Height             & 0.021277  & m         \\
    \rowcolor{black!00} Temperature difference & 10    & K         \\
    \rowcolor{black!20} Coefficient of thermal expansion & 0.00341 & $1/K$ \\
    \rowcolor{black!00} Under-relaxation u & 0.8       &           \\
    \rowcolor{black!20} Under-relaxation p & 0.2       &           \\
    \rowcolor{black!00} Under-relaxation T & 1.0       &           \\
    \rowcolor{black!20} Relative tolerance & 1E-8      &
  \end{tabular}
  \label{tab:cavity}
\end{table}

Using the data from table \ref{tab:cavity} and given Prandtl number for the flow problem \(Pr = 0.71\) the Rayleigh number \(Ra\), a dimensionless number used to characterize buoyancy driven flows, can be calculated. From the definition of the Prandtl number the thermal diffusivity \(\alpha\) can be calculated as
\begin{displaymath}
  \alpha = \frac{\mu}{\rho \, Pr} = 2.1304 * 10^{-5}.
\end{displaymath}
It follows that the Rayleigh number can be determined to 
\begin{displaymath}
  Ra = \frac{\rho g \beta \Delta T h^3}{\mu \alpha} \approx 10^4,
\end{displaymath}
which implies according to \cite{christon02} that the flow is still stationary and non-turbulent.

The tests were conducted on the formerly presented HHLR cluster, using the MPI2 section. Table \ref{tab:cavitycompare} compares the wall-clock times and number of  for different solver and coupling approaches. The presented results are in good agreement with \cite{vakilipour12}, which show monotonic decrease of the number of iterations with increased implicit coupling. It is notable that the pressure velocity coupling is responsible for the biggest decrease in the number of non-linear iterations. Different to the results presented in section \ref{sec:channel} this does not yield significant performance benefits with respect to wall-clock time.  In order to achieve the benefits of a fully coupled solution algorithm the coupling has to be increased to also involve semi-implicit temperature-to-velocity/pressure and implicit velocity-to-temperature coupling. 

\begin{table}[h!]\centering
\ra{1.3}
  \caption{Performance analysis results of the heated cavity flow problem comparing the SIMPLE algorithm with segregated temperature solve (SEG), the fully coupled solution algorithm with segregated temperature solve (CPLD), the fully coupled solution algorithm with an implicit Boussinesq approximation (TCPLD) and the fully coupled solution algorithm using an implicit Boussinesq approximation and a semi-implicit Newton-Raphson linearization of the convective part of the temperature equation (NRCPLD).}
  \begin{tabular}{cccc}\toprule
    Resolution & Solver configuration & Time & No. Non-linear its. \\
    \midrule
    \rowcolor{black!20}\multirow{4}{*}{}            & SEG    & 0.3719E+02 & 203 \\
    \rowcolor{black!20}                             & CPLD   & 0.6861E+02 & 62  \\
    \rowcolor{black!20}                             & TCPLD  & 0.1012E+03 & 31  \\
    \rowcolor{black!20} \multirow{-4}{*}{32x32x32}  & NRCPLD & 0.2153E+02 & 22  \\ %\hline
    %
    \rowcolor{black!00}\multirow{4}{*}{}            & SEG    & 0.1997E+04 &  804 \\
    \rowcolor{black!00}                             & CPLD   & 0.7687E+03 &  63  \\
    \rowcolor{black!00}                             & TCPLD  & 0.1278E+04 &  59  \\
    \rowcolor{black!00} \multirow{-4}{*}{64x64x64}  & NRCPLD & 0.4240E+03 &  17  \\ %\hline
    %
    \rowcolor{black!20}\multirow{4}{*}{}               & SEG    & 0.5197E+05 &  3060 \\
    \rowcolor{black!20}                                & CPLD   & 0.1860E+05 &  74   \\
    \rowcolor{black!20}                                & TCPLD  & 0.1950E+05 &  50   \\
    \rowcolor{black!20} \multirow{-4}{*}{128x128x128}  & NRCPLD & 0.6155E+04 &  18   \\ %\hline
    %
    %\rowcolor{black!00}\multirow{4}{*}{}               & SEG    &  & \\
    %\rowcolor{black!00}                                & CPLD   &  & \\
    %\rowcolor{black!00}                                & TCPLD  &  & \\
    %\rowcolor{black!00} \multirow{-4}{*}{256x256x256}  & NRCPLD &  & \\ %\hline
  \end{tabular}
  \label{tab:cavitycompare}
\end{table}

Furthermore it can be noticed that the solely use of velocity-to-temperature coupling does not result in benefits compared to the coupled solution for velocities and pressure combined with a decoupled solve for the temperature equation. This is accredited to the treatment of the non-linearity of the temperature equation in the TCPLD solver configuration. Even though the momentum balances implicitly use the temperature from the next iteration, the temperature equation does not use the velocities of the next iteration. Instead the convective fluxes are calculated with the velocities from the previous iteration. Even though the implicit velocity-to-temperature coupling reduces the number of needed non-linear iterations an increase in the needed wall-clock time for computation can be noticed. This is attributed to the augmented costs during the application of the linear solver algorithm as a result of the additional degree of freedom that the linear system embraces. The necessary number of outer iterations is higher than in the NRCPLD configuration since the convective fluxes in the temperature equation are not coupled to the momentum balances which degrades the convergence of the non-linear iteration process.

It can be concluded that in order to profit from the benefits of implicit coupling the key lays in the semi-implicit temperature-to-velocity/pressure coupling. However, it should be noted that the implicit consideration of the corresponding terms furthermore increases the amount of memory needed for computation and hence does not come without drawbacks.

